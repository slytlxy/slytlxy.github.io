# further research about universal sentence encoder

basically we are giving up the plan, since the training process includes multi-task learning, which is too complicated to reproduce.    
https://arxiv.org/pdf/1803.11175.pdf

## transformer

+ input: PTB tokenized lowercased string
    1. t2t encoding
        - context aware representations of words in sentence
    2. compute element-wise sum of representations at each word position
+ output: sentence embedding of length 512

## deep averaging network (DAN)

+ input: PTB tokenized lowercased string
    1. take average of input word embeddings & 2-gram
    2. DNN
+ output: sentence embedding of length 512

## existing problem

+ tokenization for chn chars can be tricky, cuz no space in corpus can split word
+ multi-task learning - so hard to reproduce
